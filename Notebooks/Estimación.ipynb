{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **2. Estimación y Selección**"
      ],
      "metadata": {
        "id": "V9qdTWwePpnn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "\n",
        "def obtener_datos_fase1(ruta_archivo):\n",
        "    df = pd.read_csv(ruta_archivo)\n",
        "    df['Date'] = pd.to_datetime(df['Date'])\n",
        "    df = df.sort_values('Date')\n",
        "    serie = df['Monthly Mean Total Sunspot Number'].values\n",
        "    return serie\n",
        "try:\n",
        "    serie_solar = obtener_datos_fase1('/content/train.csv')\n",
        "except:\n",
        "    pass"
      ],
      "metadata": {
        "id": "Ep_PkoCIKi2Z"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Los pasos a seguir son los siguientes:\n",
        "\n",
        "1. Diferenciación Manual\n",
        "\n",
        "2. Cálculo de Errores\n",
        "\n",
        "3. Cálculo del Gradiente\n",
        "\n",
        "4. Algoritmo de Estimación\n",
        "\n",
        "5. Criterio de Selección (AIC)\n",
        "\n",
        "6. Ejecución y Selección Final (Selección)"
      ],
      "metadata": {
        "id": "uPIoP9fMP11H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Diferenciación Manual\n",
        "\n",
        "Ante la evidencia de un comportamiento no estacionario en la media de la serie original $\\{Z_t\\}$, asociado a la presencia de raíces unitarias, resulta imprescindible aplicar una transformación lineal mediante el método de diferenciación para inducir la estacionariedad.\n",
        "\n",
        "Para formalizar este proceso, se define el **operador de retardo** $B$ tal que su aplicación sobre una observación en el tiempo $t$ resulta en el valor del periodo inmediatamente anterior:\n",
        "\n",
        "$$B z_t = z_{t-1}$$\n",
        "\n",
        "A partir de este, se establece el **operador diferencia** $\\nabla$, definido como:\n",
        "\n",
        "$$\\nabla z_t = (1 - B)z_t = z_t - z_{t-1}$$\n",
        "\n",
        "Para generalizar este procedimiento a un proceso integrado de orden $d$, la serie transformada estacionaria $\\{y_t\\}$ se obtiene mediante la aplicación sucesiva de este operador:\n",
        "\n",
        "$$y_t = \\nabla^d z_t = (1 - B)^d z_t$$\n",
        "\n",
        "La aplicación de esta transformación elimina las tendencias estocásticas presentes en el proceso generador de datos, produciendo una nueva serie que satisface las condiciones de estabilidad requeridas para la estimación paramétrica."
      ],
      "metadata": {
        "id": "GqEZ9ScR0GsF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def diferenciar_manual(datos, d):\n",
        "    serie = np.array(datos, dtype=float)\n",
        "    for _ in range(d):\n",
        "        n = len(serie)\n",
        "        nueva_serie = np.zeros(n - 1)\n",
        "        for t in range(1, n):\n",
        "            nueva_serie[t-1] = serie[t] - serie[t-1]\n",
        "        serie = nueva_serie\n",
        "    return serie"
      ],
      "metadata": {
        "id": "xh8iXkYjKk7F"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Cálculo de Errores y Definición del Modelo ARMA\n",
        "\n",
        "Una vez garantizada la condición de estacionariedad en la serie, el proceso estocástico resultante $\\{z_t\\}$ se representa mediante un modelo Autorregresivo de Medias Móviles, denotado como $ARMA(p,q)$. La estructura lineal que rige el comportamiento temporal de la serie se define formalmente como:\n",
        "\n",
        "$$z_t = c + \\sum_{i=1}^{p}\\phi_{i}z_{t-i} + a_t - \\sum_{j=1}^{q}\\theta_{j}a_{t-j}$$\n",
        "\n",
        "Donde:\n",
        "* $\\phi_i$ son los coeficientes autorregresivos.\n",
        "* $\\theta_j$ son los coeficientes de medias móviles.\n",
        "* $c$ es la constante del modelo.\n",
        "* $\\{a_t\\}$ representa el proceso de innovaciones (ruido blanco) con media cero.\n",
        "\n",
        "Para proceder con la estimación de los parámetros, es necesario aislar el término de error para cuantificar la discrepancia entre la observación real y el valor esperado por el modelo. Reordenando los términos de la ecuación general, la innovación en el tiempo $t$ se expresa como:\n",
        "\n",
        "$$a_t = z_t - \\left( c + \\sum_{i=1}^{p}\\phi_{i}z_{t-i} - \\sum_{j=1}^{q}\\theta_{j}a_{t-j} \\right)$$\n",
        "\n",
        "El cálculo de esta secuencia de residuos se efectúa de manera recursiva. Dado que las innovaciones pasadas son desconocidas al inicio del periodo de observación, se adopta el supuesto de **estimación condicional**, estableciendo la condición inicial de que $a_t = 0$ para todo $t \\le 0$. La suma de los cuadrados de estos errores calculados ($SSE$) constituye la función objetivo que se buscará minimizar posteriormente."
      ],
      "metadata": {
        "id": "iyNEUf882gtf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calcular_sse_manual(params, serie, p, q):\n",
        "    T = len(serie)\n",
        "    c = params[0]\n",
        "    phis = params[1 : p+1]\n",
        "    thetas = params[p+1 : p+1+q]\n",
        "\n",
        "    a = np.zeros(T)\n",
        "    sse = 0.0\n",
        "    k = max(p, q)\n",
        "\n",
        "    for t in range(k, T):\n",
        "        comp_ar = 0.0\n",
        "        for i in range(p):\n",
        "            comp_ar += phis[i] * serie[t - (i + 1)]\n",
        "\n",
        "        comp_ma = 0.0\n",
        "        for j in range(q):\n",
        "            comp_ma += thetas[j] * a[t - (j + 1)]\n",
        "\n",
        "        prediccion = c + comp_ar - comp_ma\n",
        "        error = serie[t] - prediccion\n",
        "        a[t] = error\n",
        "        sse += error**2\n",
        "\n",
        "    return sse"
      ],
      "metadata": {
        "id": "WOgJMp9sKqoo"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Cálculo del Gradiente\n",
        "\n",
        "El problema de estimación de los parámetros del modelo $\\beta = (c, \\phi, \\theta)$ se formula como un problema de optimización no lineal, cuyo objetivo es encontrar el vector de coeficientes que minimice la función de pérdida definida por la suma de cuadrados de los residuos, $S(\\beta)$.\n",
        "\n",
        "Para resolver este problema mediante métodos iterativos de optimización, es indispensable determinar la dirección de máximo descenso de la función objetivo en el espacio de parámetros. Esta dirección está dada por el vector gradiente $\\nabla S(\\beta)$, el cual se compone de las derivadas parciales de la función de pérdida respecto a cada uno de los coeficientes del modelo:\n",
        "\n",
        "$$\\nabla S(\\beta) = \\left[ \\frac{\\partial S}{\\partial c}, \\frac{\\partial S}{\\partial \\phi_1}, \\dots, \\frac{\\partial S}{\\partial \\phi_p}, \\frac{\\partial S}{\\partial \\theta_1}, \\dots, \\frac{\\partial S}{\\partial \\theta_q} \\right]^T$$\n",
        "\n",
        "Dada la complejidad algebraica derivada de la recursividad en los términos de media móvil, la evaluación analítica de estas derivadas resulta impracticable. Por consiguiente, se emplea el método de aproximación numérica por diferencias finitas. La derivada parcial respecto al $i$-ésimo parámetro se aproxima mediante el cociente incremental:\n",
        "\n",
        "$$\\frac{\\partial S}{\\partial \\beta_i} \\approx \\frac{S(\\beta + h \\cdot e_i) - S(\\beta)}{h}$$\n",
        "\n",
        "Donde $h$ representa una perturbación infinitesimal y $e_i$ es el vector unitario en la dirección del parámetro $i$. Este cálculo permite cuantificar la sensibilidad del error ante variaciones marginales en los coeficientes, proporcionando la información necesaria para actualizar las estimaciones."
      ],
      "metadata": {
        "id": "MChRh7Is3V2k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calcular_gradiente_manual(params, serie, p, q):\n",
        "    epsilon = 1e-5\n",
        "    grads = np.zeros(len(params))\n",
        "    base_sse = calcular_sse_manual(params, serie, p, q)\n",
        "\n",
        "    for i in range(len(params)):\n",
        "        p_copy = params.copy()\n",
        "        p_copy[i] += epsilon\n",
        "        new_sse = calcular_sse_manual(p_copy, serie, p, q)\n",
        "        grads[i] = (new_sse - base_sse) / epsilon\n",
        "\n",
        "    return grads"
      ],
      "metadata": {
        "id": "xLypgg0tKsN9"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Algoritmo de Estimación\n",
        "\n",
        "La estimación de los coeficientes del modelo se realiza bajo el principio de Máxima Verosimilitud. Asumiendo que los errores siguen una distribución normal, la maximización de la función de verosimilitud es equivalente a la minimización de la Suma de Cuadrados de los Residuos ($SSR$). Se define el problema de optimización como:\n",
        "\n",
        "$$\\min_{\\beta} S(\\beta) = \\sum_{t=1}^{T} a_t^2(\\beta)$$\n",
        "\n",
        "Donde $\\beta$ es el vector de parámetros desconocidos. Para resolver este problema de optimización no lineal, se implementa el algoritmo iterativo de **Descenso de Gradiente**. Este método busca el mínimo local de la función de costo actualizando los parámetros en la dirección opuesta al gradiente calculado. La regla de actualización en cada iteración $k$ está dada por:\n",
        "\n",
        "$$\\beta_{k+1} = \\beta_k - \\eta \\cdot \\nabla S(\\beta_k)$$\n",
        "\n",
        "Donde $\\eta$ representa la tasa de aprendizaje, un hiperparámetro que controla la magnitud del ajuste en cada paso. El proceso iterativo continúa hasta que se alcanza un criterio de convergencia predefinido, momento en el cual se obtienen los estimadores de mínimos cuadrados condicionales que mejor ajustan el modelo a los datos observados."
      ],
      "metadata": {
        "id": "4AHzwVWw4ICz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def estimar_parametros_manual(serie, p, q, lr=1e-9, max_iter=1000):\n",
        "    n_params = 1 + p + q\n",
        "    params = np.zeros(n_params) + 0.001\n",
        "\n",
        "    for _ in range(max_iter):\n",
        "        grad = calcular_gradiente_manual(params, serie, p, q)\n",
        "        params = params - (lr * grad)\n",
        "\n",
        "    sse_final = calcular_sse_manual(params, serie, p, q)\n",
        "    return params, sse_final"
      ],
      "metadata": {
        "id": "_maFskfXKvsZ"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Criterio de Selección del Modelo (AIC)\n",
        "\n",
        "Para la discriminación objetiva entre los distintos modelos candidatos y la determinación del orden óptimo $(p, q)$, se utiliza el Criterio de Información de Akaike (AIC). Este índice estadístico proporciona una medida de la calidad relativa del modelo, fundamentada en la teoría de la información, estableciendo un balance entre la bondad de ajuste y la complejidad del mismo (principio de parsimonia).\n",
        "\n",
        "La formulación matemática del criterio se define mediante la siguiente expresión logarítmica:\n",
        "\n",
        "$$AIC = T \\ln(\\hat{\\sigma}^2) + 2k$$\n",
        "\n",
        "Donde:\n",
        "* $T$ representa el número de observaciones efectivas utilizadas en la estimación.\n",
        "* $\\hat{\\sigma}^2$ es el estimador de máxima verosimilitud de la varianza del proceso de ruido blanco, calculado a partir de la suma de cuadrados de los residuos ($SSE$) como $\\hat{\\sigma}^2 = \\frac{SSE}{T}$.\n",
        "* $k$ denota el número total de parámetros estimados en el modelo ($p + q + 1$, incluyendo la constante).\n",
        "\n",
        "El criterio penaliza la inclusión de parámetros adicionales mediante el término $2k$, compensando la reducción en la varianza del error y evitando así el sobreajuste del modelo a la serie de datos."
      ],
      "metadata": {
        "id": "QbD1Cded4-Eo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calcular_aic(sse, n_obs, n_params):\n",
        "    if sse <= 0: return float('inf')\n",
        "    sigma2 = sse / n_obs\n",
        "    aic = n_obs * math.log(sigma2) + 2 * n_params\n",
        "    return aic"
      ],
      "metadata": {
        "id": "TEs71p6iKyp-"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Ejecución y Selección Final\n",
        "\n",
        "La etapa final del procedimiento de modelación estocástica integra los resultados de la estimación paramétrica para determinar el modelo óptimo que mejor describa el proceso generador de los datos. Esta fase se sustenta en la evaluación comparativa de los modelos candidatos sugeridos durante la etapa de identificación, específicamente las estructuras $AR(2)$, $AR(3)$, $AR(4)$ y $ARMA(2,1)$.\n",
        "\n",
        "El criterio de decisión se fundamenta en el principio de parsimonia y la minimización de la pérdida de información de Kullback-Leibler. Para cada modelo candidato $M_i$ con estructura $(p, d, q)$, se calculan sus parámetros de máxima verosimilitud y el correspondiente valor del Criterio de Información de Akaike ($AIC_i$).\n",
        "\n",
        "La regla de decisión estricta para la selección del modelo ganador se define formalmente como:\n",
        "\n",
        "$$M_{óptimo} = \\arg \\min_{M_i \\in \\Omega} \\{ AIC(M_i) \\}$$\n",
        "\n",
        "Donde $\\Omega$ representa el conjunto de modelos candidatos evaluados. Aquel modelo que minimice el valor del AIC será seleccionado como el más adecuado, garantizando un equilibrio óptimo entre la bondad de ajuste (reducción de la varianza residual) y la complejidad del modelo (número de parámetros estimados)."
      ],
      "metadata": {
        "id": "qWWD2aQI4-0K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ejecutar_seleccion_final(datos_crudos):\n",
        "\n",
        "    candidatos = [\n",
        "        (2, 0, 0),\n",
        "        (3, 0, 0),\n",
        "        (4, 0, 0),\n",
        "        (2, 0, 1)\n",
        "    ]\n",
        "\n",
        "    mejor_aic = float('inf')\n",
        "    mejor_modelo = None\n",
        "    mejor_params = None\n",
        "\n",
        "    print(f\"{'MODELO':<15} | {'SSE':<12} | {'AIC':<12}\")\n",
        "    print(\"-\" * 45)\n",
        "\n",
        "    for p, d, q in candidatos:\n",
        "        try:\n",
        "            datos_trabajo = diferenciar_manual(datos_crudos, d)\n",
        "\n",
        "            params, sse = estimar_parametros_manual(datos_trabajo, p, q)\n",
        "\n",
        "            k = len(params)\n",
        "            T_eff = len(datos_trabajo) - max(p, q)\n",
        "            aic_val = calcular_aic(sse, T_eff, k)\n",
        "\n",
        "            print(f\"ARIMA({p},{d},{q})    | {sse:.2f}   | {aic_val:.4f}\")\n",
        "\n",
        "            if aic_val < mejor_aic:\n",
        "                mejor_aic = aic_val\n",
        "                mejor_modelo = (p, d, q)\n",
        "                mejor_params = params\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "    print(\"-\" * 45)\n",
        "    print(f\"MODELO GANADOR: ARIMA{mejor_modelo}\")\n",
        "    print(f\"AIC Mínimo: {mejor_aic:.4f}\")\n",
        "\n",
        "    print(f\"PARÁMETROS ESTIMADOS:\")\n",
        "    nombres = ['Constante (c)'] + [f'AR({i+1})' for i in range(mejor_modelo[0])] + [f'MA({i+1})' for i in range(mejor_modelo[2])]\n",
        "\n",
        "    for nombre, valor in zip(nombres, mejor_params):\n",
        "        print(f\"{nombre:<15}: {valor:.6f}\")\n",
        "\n",
        "if 'serie_solar' in locals():\n",
        "    ejecutar_seleccion_final(serie_solar)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3v6iHcQ0K1Su",
        "outputId": "15916694-7267-4f56-ceaf-4281991d064a"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MODELO          | SSE          | AIC         \n",
            "---------------------------------------------\n",
            "ARIMA(2,0,0)    | 2081274.16   | 19276.9864\n",
            "ARIMA(3,0,0)    | 2001911.90   | 19159.3167\n",
            "ARIMA(4,0,0)    | 1957274.64   | 19089.6307\n",
            "ARIMA(2,0,1)    | 2111822.40   | 19321.7668\n",
            "---------------------------------------------\n",
            "MODELO GANADOR: ARIMA(4, 0, 0)\n",
            "AIC Mínimo: 19089.6307\n",
            "PARÁMETROS ESTIMADOS:\n",
            "Constante (c)  : 0.010931\n",
            "AR(1)          : 0.575530\n",
            "AR(2)          : 0.158144\n",
            "AR(3)          : 0.102603\n",
            "AR(4)          : 0.147206\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conclusiones de la Etapa de Estimación\n",
        "\n",
        "A partir de la ejecución del algoritmo de optimización y el análisis comparativo de los modelos candidatos, se determinó que el modelo **ARIMA(4,0,0)** minimiza de manera efectiva el Criterio de Información de Akaike (AIC), alcanzando un valor de **19,089.63**, el menor entre todas las especificaciones evaluadas.\n",
        "\n",
        "Este resultado empírico valida la hipótesis planteada en la fase de **Identificación** respecto a la estructura de dependencia de la serie:\n",
        "\n",
        "1.  **Validación del Límite Superior:** Durante la identificación, se sugirió el modelo $AR(4)$ como un \"límite superior\" debido a que el cuarto rezago se encontraba en la frontera de las bandas de significancia de la FAP. La estimación paramétrica confirma que la inclusión del término $\\phi_4$ (estimado en $0.147$) aporta una reducción significativa en la varianza del error que compensa la penalización por complejidad del AIC.\n",
        "2.  **Superioridad frente al AR(3):** Aunque el modelo $AR(3)$ captura gran parte de la estructura, la diferencia en el AIC respecto al modelo ganador ($19,159$ vs $19,089$) indica que truncar el modelo en el tercer rezago conlleva una pérdida de información relevante sobre el ciclo de las manchas solares.\n",
        "3.  **Ineficacia del Componente de Media Móvil:** El modelo $ARMA(2,1)$ presentó el peor desempeño ($AIC \\approx 19,321$). Esto corrobora que la persistencia de la serie se modela más adecuadamente mediante una estructura autorregresiva pura de mayor orden, en lugar de corregir mediante términos de media móvil.\n",
        "\n",
        "En conclusión, se selecciona el modelo **ARIMA(4,0,0)** como la especificación óptima para proceder a las siguientes pruebas."
      ],
      "metadata": {
        "id": "bZJ50MVvBXia"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}